---
title: A Segmented Approach to Predicting Prediction Model Performance After Transfer Using Unlabelled Data
author: Arman Norouzi
bibliography: cite.bib
csl: bmcemerg.csl
---

```{r, echo = FALSE}
knitr::opts_chunk$set(comment = NA)
library(rio)
library(dplyr)
library(MASS)
library(tableone)
library(survival)
library(dplyr)
library(boot)
library(ggplot2)
library(knitr)
library(caret)
url <- "https://datadryad.org/stash/downloads/file_stream/30857"
raw.data <- import(url, format = "xls") %>% as.data.frame()


#------------------------------------------
pctabledf <- data.frame(raw.data)

listk <- c(2, 3, 5, 7, 8, 10, 11, 16, 17)
listc <- c()
for (i in listk) {
    listc <- c(listc, colnames(pctabledf[i]))
}

pctabledf <- subset(pctabledf, select = listc)
colnames(pctabledf) <- c('Country', 'Respiratory rate (per min)', 'Gender', 'Peripheral oxygen saturation (%)', 'Systolic blood pressure (mm Hg)', 'Pulse (bpm)', 'Temperature (°C)', 'Age', 'ICU admission')

pctabledf$`ICU admission`[pctabledf$`ICU admission` == 1] <- 'Admission'
pctabledf$`ICU admission`[pctabledf$`ICU admission` == 0] <- 'No admission'

pctabledf$Gender[pctabledf$Gender == 'f'] <- 'Female'
pctabledf$Gender[pctabledf$Gender == 'm'] <- 'Male'

strata <- "Country"
vars <- colnames(pctabledf)[!(colnames(pctabledf) %in% strata)]

listk <- c(2,3,4,6,7)
listc <- c()
for (i in listk) {
    listc <- c(listc, colnames(pctabledf)[i])
}
biomarkers <- c(listc)

Pctable <- CreateTableOne(vars = vars, data = pctabledf, strata = strata, test = FALSE, addOverall = TRUE)

#------------------------------------------


## This function creates a simulated dataset, i.e. simulates data for
## each level of a given strata variables and combines these data into
## a single data.frame. The size is for each level of strata.
create_simulated_dataset <- function(raw.data, strata, outcome, predictors, size = 10000) {
    data.list <- split(raw.data, f = as.factor(raw.data[, strata]))
    simulated.data.list <- lapply(data.list, simulate_data,
                                  outcome = outcome, predictors = predictors,
                                  size = size)
    simulated.data.list <- lapply(names(data.list), function(name) {
        dataset <- simulated.data.list[[name]]
        dataset$strata <- name
        dataset
    })
    simulated.dataset <- do.call(rbind, simulated.data.list) %>% as.data.frame()
    simulated.dataset
}

## This function simulates a dataset with continuous predictors and a
## binary outcome. It achieves this based on a "template" dataset
## using its covariance matrix and fitting a logistic regression model
## with the true data. 
simulate_data <- function(dataset, outcome, predictors, size) {
    y <- dataset[, outcome]
    x <- dataset[predictors]
    fit <- glm(y ~ ., family = binomial, data = cbind(y, x))
    cov.matrix <- cov(x)
    sim.data <- mvrnorm(size, sapply(x, mean), cov.matrix, empirical = TRUE) %>% as.data.frame()
    sim.data$yhat <- predict(fit, newdata = sim.data, type = "response")
    sim.data$y <- rbinom(nrow(sim.data), 1, prob = sim.data$yhat)
    sim.data
}

## This function estimates the performance of each approach in a
## specific strata combination, as well as the differences between
## approaches
estimate_performance <- function(strata.combination, strata, df) {
    devsample <- df[df[, strata] == strata.combination[1], ]
    valsample <- df[df[, strata] == strata.combination[2], ]

    devsample$`ICU admission`[devsample$`ICU admission` == 1] <- 'Admission'
    devsample$`ICU admission`[devsample$`ICU admission` == 0] <- 'No admission'
    
    valsample$`ICU admission`[valsample$`ICU admission` == 1] <- 'Admission'
    valsample$`ICU admission`[valsample$`ICU admission` == 0] <- 'No admission'
    
    #Adjusting for overfitting with oneSE caret package
    train.control <- trainControl(method = 'cv', number = 5, selectionFunction = 'oneSE')

    #logreg is now our prediction model that is chosen through cross-validation 80-20 ?
    logreg <- train(`ICU admission` ~ `Systolic blood pressure (mm Hg)` + `Pulse (bpm)` + `Temperature (°C)` + `Peripheral oxygen saturation (%)` + `Respiratory rate (per min)`, 
                    data = devsample, method = 'glm', trControl = train.control)
    
    # x calcualted as correct prediction in the development sample
    probabilities <- predict(logreg, newdata = devsample, type = 'prob')$Admission
    predict.classesdev <- ifelse(probabilities > 0.5, 'Admission', 'No admission')

    x <- mean(predict.classesdev == devsample$`ICU admission`) * 100
    
    
    # y calculated as the correct predictions in the validation sample
    probabilities <-predict(logreg, newdata = valsample, type = 'prob')$Admission
    predict.classesval <- ifelse(probabilities > 0.5, 'Admission', 'No admission')
    
    y <- mean(predict.classesval == valsample$`ICU admission`) * 100

    ## Assign data "origin" as new variable and combine data
    devsample['devval'] <- 1
    valsample['devval'] <- 0

    df_pooled <- rbind(devsample, valsample)

    ## Create propensity model
    logregi <- glm(`devval` ~ `Age` + `Systolic blood pressure (mm Hg)` + `Pulse (bpm)` + `Temperature (°C)` + `Peripheral oxygen saturation (%)` + `Respiratory rate (per min)`, 
                   data = df_pooled, family = binomial)
    probabilities <- logregi %>% predict(df_pooled, type = "response")
    predict.classespool <- ifelse(probabilities > 0.5, 1, 0)

    ## Identify the segment of observation in the development data that
    ## are "most similar" to the observations in the validation data
    missmatch <- predict.classespool == 0 & df_pooled$devval == 1
    df_segment <- devsample[missmatch, ]

    # z calculated as the correct predictions in the segment created
    df_segment$`ICU admission`[df_segment$`ICU admission` == 1] <- 'Admission'
    df_segment$`ICU admission`[df_segment$`ICU admission` == 0] <- 'No admission'
    
    probabilities <- predict(logreg, newdata = df_segment, type = 'prob')$Admission
    predict.classessegment <- ifelse(probabilities > 0.5, 'Admission', 'No admission')
    
    z <- mean(predict.classessegment == df_segment$`ICU admission`) * 100

    ## Redefined some of these measures. Now a positive diff_diff
    ## means that the segmented approach works better than the naive
    ## approach
    pe_dev <- x
    pe_tval <- y
    pe_pval <- z
    pe_tval_dev <- abs(x - y)
    pe_pval_dev <- abs(z - y)
    pe_diff_diff <- pe_tval_dev - pe_pval_dev 

    ## We are interested in the "error" associated with each
    ## approach. By taking the absolute difference we weigh positive
    ## and negative errors equally, i.e. we say that it is equally
    ## wrong to overestimate accuracy as it is to underestimate. This
    ## simplifies the calculations. Consider the following example, if
    ## the accuracy of model A is 0.7 in the development sample and
    ## the true accuracy in the validation sample is 0.6. Then the
    ## error is 0.1. If the accuracy in the segmented sample is 0.75
    ## then the error of this approach is -.05. When we are
    ## calculating the difference in error we get -0.15, which is not
    ## what we want, because we can see that the difference in error
    ## is really 0.05. One easy solution is then to work with absolute
    ## differences. Hope this makes sense.
    
    stats <- c(pe_dev = pe_dev,
               pe_tval = pe_tval,
               pe_pval = pe_pval,
               pe_tval_dev = pe_tval_dev,
               pe_pval_dev = pe_pval_dev,
               pe_diff_diff = pe_diff_diff)
    return (stats)
}
listdf <- c()

## This function runs one simulation, and returns all accuracys and differences along with the dataframe simulated
run_simulation <- function(raw.data, strata, strata.combinations, outcome, predictors, size = 10000) {
    df <- create_simulated_dataset(raw.data, strata, outcome, predictors, size = size)
    new.colnames <- c(strata = "Country",
                      age = "Age",                  
                      resp_rate = "Respiratory rate (per min)",
                      SpO2 = "Peripheral oxygen saturation (%)",
                      BPS = "Systolic blood pressure (mm Hg)",
                      HR = "Pulse (bpm)",
                      temp = "Temperature (°C)",
                      y = "ICU admission")
    df <- df[names(new.colnames)]
    colnames(df) <- new.colnames
    strata <- "Country"
    vars <- colnames(df)[!(colnames(df) %in% strata)]
    df$`ICU admission` <- as.numeric(df$`ICU admission`)
    df_USA <- df[df$Country == "USA", ]
    df_France <- df[df$Country == "France", ]
    df_Swizerland <- df[df$Country == "Switzerland", ]
    performance.estimates <- lapply(strata.combinations, estimate_performance, strata = strata, df = df)
    names(performance.estimates) <- sapply(strata.combinations, paste0, collapse = ".to.")
    results <- unlist(performance.estimates)
    results <- c(results, data.frame(df))
    return (results)

}


## Okay, so instead of creating one simulated dataset and bootstrapping
## that I've revised the code to repeat the simulation process
## multiple times and then using the mean across simulations as the
## point estimate and the 2.5% and 97.5% percentiles as measures of
## uncertainty
strata <- "country"
predictors <- c("resp_rate", "SpO2", "BPS", "HR", "temp", "age")
outcome <- "ICU"
size <- 10000
combinations <- expand.grid(rep(list(unique(raw.data[, strata])), 2))
strata.combinations <- t(combinations[combinations$Var1 != combinations$Var2, ]) %>% as.data.frame()
n.simulations <- 10
simulation.results <- lapply(seq_len(n.simulations), function(i) run_simulation(
                                                                     raw.data = raw.data,
                                                                     strata = strata,
                                                                     strata.combinations = strata.combinations,
                                                                     outcome = outcome,
                                                                     predictors = predictors,
                                                                     size = size))

# This gives us the dataframe for each of the simulations and the accuracys in (simulation.restults1 and list.simulated.dfs) 
list.simulated.dfs <- data.frame()
simulation.results1 <- c()
for (i in 1:n.simulations) {
    simulation.results1 <- c(simulation.results1, list(unlist(simulation.results[[i]][1:36])))
    list.simulated.dfs <- rbind(list.simulated.dfs, simulation.results[[i]][37:44] %>% as.data.frame())
}

#accuracys as we did before
simulation.results <- simulation.results1
simulation.data <- do.call(rbind, simulation.results)
pe <- colMeans(simulation.data)
cis <- lapply(as.data.frame(simulation.data), function(x) {
    quantiles <- quantile(x, probs = c(0.025, 0.975))
    names(quantiles) <- NULL
    c(lb = quantiles[1], ub = quantiles[2])
}) %>% as.data.frame() %>% t()
pes.with.cis <- cbind(pe, cis) %>% as.data.frame() %>% split(f = as.factor(rownames(cis)))

# Creating a tableone for all simulated dataframes
colnames(list.simulated.dfs) <- c('Country', 'Age', 'Respiratory rate (per min)', 'Peripheral oxygen saturation (%)', 
                                  'Systolic blood pressure (mm Hg)', 'Pulse (bpm)', 'Temperature (°C)', 'ICU admission')

list.simulated.dfs$`ICU admission`[list.simulated.dfs$`ICU admission` == 1] <- 'Admission'
list.simulated.dfs$`ICU admission`[list.simulated.dfs$`ICU admission` == 0] <- 'No admission'

strata <- "Country"
vars <- colnames(list.simulated.dfs)[!(colnames(list.simulated.dfs) %in% strata)]

Pctablesim <- CreateTableOne(vars = vars, data = list.simulated.dfs, strata = strata, test = FALSE, addOverall = TRUE)

#function for plotting jitterplot+errorbars, n = (1,2,3,4,5,6), each number for specific accuracy (dev, tval, pval, tval-dev, pval-dev, diffdiff)
jitterplot <- function(n) {
    i = 0
    trans <- c()
    values <- c()
    cislb <- c()
    cisub <- c()
    ppe <- c()
    signi <-c()
    if (n == 1) {k = 2
    } else {k = 5}
    while (i <= k) {
        #all transfer names in a list in order to label all values with correct transfers
        trans <- c(trans, rep(colnames(simulation.data)[n+i*6], n.simulations))
        
        #all specific point estimate for certain accuracy in 1 list
        values <- c(values, simulation.data[,n+i*6])
        
        #confidence intervals to list in order to call easier in ggplot
        cislb <- c(cislb, rep(cis[n+i*6], n.simulations))
        cisub <- c(cisub, rep(cis[n+36+i*6], n.simulations))
        
        #all point estimates means added in list in order to call easier in ggplot
        ppe <- c(ppe, rep(pe[[n+i*6]], n.simulations))
        
        #assigns significance in list
        signi <- c(signi, rep(between(0, cis[n+i*6], cis[n+36+i*6]), n.simulations))
        i = i+1
    }
    
    # creating dataframe with all our important values needed to create plots
    tempdf <- data.frame('Transfers' = trans,
                         'Values' =values,
                         'Significance' = signi)
    
#converting boolean TRUE/FALSE to axtris or not for significance
    tempdf$Significance[tempdf$Significance == FALSE] <- '*'
    tempdf$Significance[tempdf$Significance == TRUE] <- ''
    
    #new xlabels and y labes and sizes
    if (n == 1) {newxlabels <- c('France', 'Switzerland', 'USA')
    } else {newxlabels <- c('France to USA', 'Switzerland to USA', 'USA to France', 'Switzerland to France', 'USA to Switzerland', 'France to Switzerland')}
    newylabels <- c('Accuracy in development sample (%)', 
                    'Accuracy in validation sample (%)', 
                    'Predicted accuracy in validation sample (%)', 
                    'Error in naive approach',
                    'Error in segmented approach',
                    'Error difference between naive and segmented approach')
    
    
    size.position.jitter <- c(0.15, 0.2)
    width.errorbar <- c(0.35, 0.5)
    xlabels <- c('Development sample', 'Transfers')
    
    if (n == 1) {
        size.position.jitter <- size.position.jitter[1]
        width.errorbar <- width.errorbar[1]
        xlabels <- xlabels[1]
    } else {
        size.position.jitter <- size.position.jitter[2]
        width.errorbar <- width.errorbar[2]
        xlabels <- xlabels[2]
    }
    
    #ploting jitterplot + errorbars + mean pointestimate and significance axtris
    p <- ggplot() +
        geom_jitter(data = tempdf, aes(x = Transfers, y = Values), alpha = 0.09, position=position_jitter(size.position.jitter)) +
        geom_errorbar(data = tempdf, aes(x = Transfers, y = Values, ymin = cislb, ymax = cisub), size = 0.1, width=width.errorbar, color = "red") +
        geom_point(data = tempdf, aes(x = Transfers, y = ppe), color= "red") +
        scale_x_discrete(breaks=c(unique(tempdf$Transfers)), 
                         labels= newxlabels) +
        ylab(newylabels[n]) +
        xlab(xlabels) +
        coord_flip() +
        theme_test() 
        if (n >= 6) {
            p <- p + geom_hline(yintercept = 0, alpha = 0.4)
            p <- p + geom_text(data = tempdf, aes(x = Transfers, y = ppe, label = Significance), nudge_x = 0.3, alpha = 0.015)
        }

    #returns our plot
    return(p)
    
}

#function for returning tableplots
dftable <- function(n) {
    #assigning trans as rownames
    trans <- c('1: Performance in development sample', 
               '2: Performance in validation sample', 
               '3: Performance in segmented sample', 
               '4: Absolute difference 2 and 1 (Naive apporach)',
               '5: Absolute difference 3 and 1 (Segmented approach)', 
               '6: Difference 5 and 4 (Approach difference)')
    i = 1
    pem <- c()
    cislb <- c()
    cisub <- c()
    signi <- c()
    stringci <- c()
    while (i <= 6) {
        #getting pointestiamte means
        pem <- c(pem, round(pe[[(n-1)*6+i]], 2))
        
        #getting confidence intervals
        cislb <- c(cislb, round(cis[[(n-1)*6+i]],2))
        cisub <- c(cisub, round(cis[[(n-1)*6+36+i]],2))
        
        #assigning if significant
        if (i >= 6) {
            signi <- c(signi, between(0, cislb[i], cisub[i]))
        } else { signi <- c(signi, '')}
        
        #stringing together a good looking CI that I can use in dataframe
        stringci <- c(stringci, paste('[', toString(cislb[i]),' - ', toString(cisub[i]), ']',toString(signi[i]), sep = ''))
        i = i+1
    }
    #changing FALSE/TRUE to axtris or not
    stringci <- gsub('FALSE', '*', stringci)
    stringci <- gsub('TRUE', '', stringci)
    
    #assigning table with specific column names
    tablep <- data.frame('Performances' = trans,
                        'Point estimate mean' = pem,
                        '95% CI' = stringci,
                        check.names = FALSE)
    
    #returning table i just created
    return(tablep)
}
#ploting different tables with knitr

#kable(dftable(1), align = 'lcc', caption = paste(strata.combinations[[1]][1], strata.combinations[[1]][2], sep = ' to '))
#kable(dftable(2), align = 'lcc', caption = paste(strata.combinations[[2]][1], strata.combinations[[2]][2], sep = ' to '))
#kable(dftable(3), align = 'lcc',caption = paste(strata.combinations[[3]][1], strata.combinations[[3]][2], sep = ' to '))
#kable(dftable(4), align = 'lcc', caption = paste(strata.combinations[[4]][1], strata.combinations[[4]][2], sep = ' to '))
#kable(dftable(5), align = 'lcc', caption = paste(strata.combinations[[5]][1], strata.combinations[[5]][2], sep = ' to '))
#kable(dftable(6), align = 'lcc', caption = paste(strata.combinations[[6]][1], strata.combinations[[6]][2], sep = ' to '))


#kableone(Pctable)


#ordning i dftable
#dftable(1)
#dftable(1)[[2]][1]

#strata.combinations[[1]][1]
#strata.combinations[[1]][2]

#dftable(2)
#strata.combinations[[2]][1]
#strata.combinations[[2]][2]

#dftable(3)
#strata.combinations[[3]][1]
#strata.combinations[[3]][2]

#dftable(4)
#strata.combinations[[4]][1]
#strata.combinations[[4]][2]

#dftable(5)
#strata.combinations[[5]][1]
#strata.combinations[[5]][2]

#dftable(6)
#strata.combinations[[6]][1]
#strata.combinations[[6]][2]

#ordningen i jitterplot
#strata.combinations[[5]][1]
#strata.combinations[[5]][2]

#strata.combinations[[3]][1]
#strata.combinations[[3]][2]

#strata.combinations[[2]][1]
#strata.combinations[[2]][2]

#strata.combinations[[4]][1]
#strata.combinations[[4]][2]

#strata.combinations[[1]][1]
#strata.combinations[[1]][2]

#strata.combinations[[6]][1]
#strata.combinations[[6]][2]

#strata.combinations
```
# Abstract
*Introduction:* To help health care professionals make informed decisions, prediction models have been implemented in health care. These models usually perform worse when they are transferred to another country. At present, no method exists that that predicts the performance of a model after transfer while utilizing unlabeled data. Therefore, we have developed and tested such a method.
*Method:* We used a public dataset that included participants from three different countries to simulate new samples in each country. These samples were used to develop a prediction model in each country to then assess the accuracy of the model in the country it was developed in, in the country after transfer and in a segment identified to have more similar model predictors as the transfer country. The accuracies were then compared, and the whole process was repeated 1000 times to develop confidence intervals.
*Results:* We found that our method was significantly better at predicting the accuracy after transfer than the accuracy in the country in which the model was developed, in two of our six transfers. In the remaining four transfers our method was significantly worse at predicting the accuracy.
*Conclusions:* The results in this study suggest that our method is worse at predicting the accuracy after transfer than the accuracy in the country in which the model is developed. However, these results may have been due to the limitations of this study, hence why more research is required to gain a better understanding of how our method works.

# Introduction
In medicine, health care professionals are confronted with a wide range of information that needs to be processed in order to make informed clinical decisions. To help health care professionals make such informed decisions, prediction models (also referred to as prediction score or prediction rules) are used [@steyerberg2013prognosis ; @moons2009prognosis]. These prediction models can be defined as statistical algorithms that estimate the risk of a specific outcome occuring in an individual [@moons2009prognosis ; @collins2015transparent]. The prediction models are trained to identify  patterns in predictor data that has been labeled with the outcome of interest. These patterns can then be used in order to predict outcome based on new unlabeled predictor data [@deo2015machine].

The risk that is predicted by the prediction models is generally based on multiple predictors and the outcome could either be a disease (diagnostic model) or an event that will occur in the future (prognostic model) [@collins2015transparent ; @riley2013prognosis]. In a diagnostic model, the predicted risk can be used to reassure the patient that their symptoms are not caused by a serious disease, refer the patient to further testing or to initiate treatment [@collins2015transparent]. An example of a diagnostic model is the Ottawa Ankle Rules. This prediction model helps predict the risk of a fracture in the ankle or the foot, in patients with acute ankle injuries. To predict this risk, the model utilizes predictor data such as bone tenderness at different locations and the inability to bear weight on the injured foot immediately after injury and in the emergency department (ED). Based on the predicted risk, the healthcare professionals can decide whether the patient is in need of x-ray imaging [@shell1993decision].

In a prognostic model, the predicted risk can be used to choose between therapeutic options, plan lifestyle changes and to risk-stratify patients in therapeutic clinical trials [@moons2009prognosis ; @steyerberg2009practical ; @dorresteijn2011estimating ; @hayward2006multivariable]. An example of a prognostic model is the CHA2DS2-VASc score. This prediction model helps health care professionals by predicting the annual risk of developing an ischemic stroke in patients with atrial fibrillation. To predict this risk, the prediction model utilizes predictor data such as history for congestive heart failure, hypertension, age >74, diabetes, stroke/transiet ischemic attack/thromboembolism, vascular disease, age 65-74 and female sex [@lip2010refining]. Based on the predicted risk, health care professionals can decide whether a patient needs anticoagulation treatment [@kirchhof20162016].

There are many uses for prediction models within the fields of medicine, where the Ottawa Ankle Rules and the CHA2DS-VASc score are just two examples that have been implemented in clinical practice. In order to develop and implement such useful models within health care, several steps need to be carried out. These steps include model development studies, model validation studies and model impact studies [@royston2009prognosis ; @moons2012risk].

In the first step consisting of the model development study, the aim is to develop a prediction model [@royston2009prognosis]. The prediction model is developed by applying a statistical algorithm to a development sample. There are many algorithms to choose from, but usually when the development sample is small, a simpler algorithm is utilized such as logistic regression [@deo2015machine]. The development sample consists of predictor data labeled with the relevant outcome, which is used to train the algorithm in finding patterns between predictors and outcomes [@deo2015machine ; @royston2009prognosis]. When the algorithm has been trained, it usually tends to be optimistic in its predictive performance [@steyerberg2003internal]. It is therefore important to quantify such optimism through internal validation techniques [@steyerberg2009practical]. The quantified optimism can thereafter be adjusted for by applying shrinkage or penalization to the model [@steyerberg2001application].

In the second step consisting of the model validation study, the aim is to assess the predictive performance of the prediction model within a validation sample. The validation sample consists of new individuals, with outcome labeled predictor data, that differ in various ways from the individuals in the development sample. These individuals may differ in the time in which their data were collected (temporal validation) or from which country or hospital their data were collected (geographical validation). Geographical validation assesses how well the prediction model performs when the model is transferred from one hospital/country to another. Such external validation is important, due to internally validated prediction models tendency to perform worse in new sets of individuals. If the performance is poor within the validation sample, the model is of no value [@moons2012risk].

In the third and final step consisting of the model impact study, the aim is to assess the prediction models impact, ideally in a randomized trial. The impact of the model is assessed in variables such as decision making changes in health care professionals, patient health outcomes and/or cost-effectiveness of care. These impact studies are carried out to prove that the prediction model is of value in clinical practice [@moons2012risk].

Carrying out these prediction model studies can be complex as problems may arise during the time in which they are carried out. One such problem may occur during the model validation studies. In these studies, in order to obtain the predictive performance of the prediction model within the validation sample, both predictor and outcome data is required in the validation sample [@moons2012risk]. This data is not always available retrospectively and can present a problem that is both time inefficient and expensive, if the data is difficult to access when collecting it prospectively. 

This would for example be a problem if data for the Framingham Risk Score were to be collected prospectively, in order to perform a model validation study. The predictor data in this prediction model are cheap blood samples and simple demographics while the outcome data is cardiovascular disease within 10 years [@anderson1991cardiovascular]. The predictor data for this prediction model may be easily accessible but the outcome data is only accessible after 10 years of follow up.

It would therefore be desirable to have a method that can predict the predictive performance of a prediction model, after it has been transferred, by only utilizing unlabeled predictor data from the validation sample. Such a method could in theory, simplify the process of implementing prediction models in clinical practice and therefore indirectly improve decision making changes in health care professionals, patient health outcomes and/or cost-effectiveness of care. At present, no such studies nor methods exists which presents a substantial knowledge gap. Therefore, the aim of this study was to develop and test a new method for predicting prediction model performance after transfer utilizing unlabeled data.

# Aim
The aim of this study was to develop and test a new method for predicting prediction model performance after transfer utilizing unlabeled data. 

Our hypothesis was that our method would predict the accuracy after transfer as good or better than the accuracy in the country in which the prediction model is developed. This hypothesis is based on the fact that our methods predicted accuracy is derived from how well the prediction model predicts outcome in a segmented sample. The segmented sample includes participants from the country in which the prediction model is developed in, but only the participants that have model predictors that are more similar to the model predictors in the transfer country. More similar model predictors should theoretically result in more similar model outcomes and predictions, hence our hypothesis.

# Methods and Materials
## Study design
The study design was an analytical registry based study. To perform the analysis, a dataset was used that has been made freely reusable by the multinational observational study by Eckert A et al in the Dryad Digital Repository [@eckart2019combination ; @dryadrepos]. This dataset was chosen due to consisting of patient data from different countries with available patient parameters that can be linked to a patient outcome.

## Participants
The participants enrolled in the dataset were all patients seeking ED care between March 2013 and October 2014 within three tertiary care centers in the USA (Clearwater Hospital), France (Hôspital de la Salpêtrière) and Switzerland (Kontonsspital Aaura). The data that was registered for each participant included the hospital and the country in which the patient seeked ED care, vital signs, laboratory assessments, age, discharge location, length of stay, intensive care unit (ICU) admission and death within 30 days. The inclusion criteria to be enrolled in the dataset was that an initial blood sample was taken. The exclusion criteria were pediatric or surgical patient [@eckart2019combination].

## Variables
### Model predictors
The model predictors that were used from the dataset in order to develop prediction models and simulate new model predictors in the statistical analysis, included respiratory rate (per min), peripheral oxygen saturation (%), systolic blood pressure (mm Hg), heart rate (bpm), temperature (°C) and age. These model variables were all the available continuous variables that contained no missing data and that were collected in the ED. This selection strategy was based on a full model approach that is claimed to avoid overfitting and selection bias [@royston2009prognosis]. How these models were measured was not mentioned in the study that publicized them.

### Model outcomes
The model outcomes that were used from the dataset in order to develop prediction models and simulate new model outcomes in the statistical analysis included ICU admission and the country from which the patient seeked ED care. The decision to admit the patients to the ICU was left to the treating physician. ICU admission was chosen as the outcome for the prediction model due being more frequent than death within 30 days.

### Sample size
The final sample size used in this study was 1303 participants which included all of the participants from the dataset.

### Missing data
Because of the dataset already being filtered to mostly containing no missing data, a complete case analysis was carried out.

## Statistical analysis
### Dataset
The dataset previously mentioned in the study design was divided based on the country from which the participants seeked ED care (USA sample, France sample and Switzerland sample).

### Sequence of analysis
Analysis in this study was performed in the programming language R [@Rcitation]. The cutoff level for all of the prediction models were set to 0.5. The sequence of analysis performed were sample simulation, sample assignment, prediction model development, development sample accuracy, validation sample accuracy, propensity model development, predicted validation sample accuracy and approach comparison.

### Sample simulation
To increase the number of participants, 10000 new participants were simulated for each of the divided samples. The process of simulation included a model predictor simulation and a model outcome simulation. The model predictors that were used to simulate new model predictors included respiratory rate, peripheral oxygen saturation, systolic blood pressure, heart rate, temperature and age from the divided samples. The model outcomes that was used to simulate new model outcomes included ICU admission from the divided samples.

To perform the model predictor simulation for one of the divided samples, the mvrnorm function implemented in the MASS package was used [@MVRnormMass]. The function utilized the mean and the covariance of the model predictors in the divided sample to simulate new model predictors. To perform the model outcome simulation for the newly simulated model predictors, the glm function implemented in R was used to develop a logistic regression model. This model was trained with the model predictors and model outcomes from the divided sample. The model was then used to predict outcomes in the newly simulated model predictors. These predictions were set as the outcomes for the newly simulated model predictors. The simulated model predictors with its model outcomes constituted a simulated sample from one country. The model predictor simulation and model outcome simulation process was repeated until each divided sample had a simulated sample.

### Sample assignment
To simulate the transfer of a prediction model from one country to another, one of the simulated samples was noted as the development sample while one of the two remaining simulated samples was noted as the validation sample. The development sample represented data from the country in which the prediction model was created, while the validation sample represented data from the country in which the prediction model was transferred to.

### Prediction model development
In the prediction model development step, a prediction model was developed by training a logistic regression model with the development sample. To reduce the error difference between the development sample accuracy and the validation sample accuracy, we used cross-validation to chose the simplest model within one standard error from the best model. This was done using the caret package implemented in R [@Caretpackage]. The cross-validation technique was based on five folds. The model predictors that were used to train the model included respiratory rate, peripheral oxygen saturation, systolic blood pressure, heart rate, temperature and age. The model outcome that was used to train the model included ICU admission.

### Development sample accuracy
To assess the predictive performance of the prediction model within the country that it was developed in, the model developed in the prediction model development step was used to predict outcome within the development sample. The predictions where then compared with the true outcomes in the development sample in order to acquire the development sample accuracy.

### Validation sample accuracy
To assess the predictive performance of the prediction model within the country to which it was transferred to, the model developed in the prediction model development step was used to predict outcome within the validation sample. The predictions where then compared with the true outcomes in the validation sample in order to acquire the validation sample accuracy.

### Propensity model development
In the propensity model development step, the data from the development sample and the validation sample were pooled into one sample. This aggregated sample was used to develop a propensity model, also a prediction model, by training a logistic regression model with the aggregated sample. The propensity model was then used to predict the origin of the samples in the aggregated sample. Observations from the development sample that were missclassified as validation observations, were used to create a segmented sample. The model predictors that were used to train the propensity model included respiratory rate, peripheral oxygen saturation, systolic blood pressure, heart rate, temperature and age. The model outcome that was used to train the propensity model was the country in which the participant seeked ED care.

### Predicted validation sample accuracy
To assess our method's predicted predictive performance of the prediction model within the country it was transferred to, the model developed in the prediction model development step was used to predict outcome within the segmented sample created in the propensity model development step. The predictions where then compared with the true outcomes in the segmented sample in order acquire the predicted validation sample accuracy.

### Approach comparison
To assess the error in the "naive approach", the absolute difference between the development sample accuracy and the validation sample accuracy was calculated. To assess the error in the "segmented approach", the absolute difference between our method's predicted validation sample accuracy and the validation sample accuracy was calculated. To assess which apporach performed best, the difference between the naive approach and the segmented approach was calculated.

### Sequence repetition
To obtain 95% confidence intervals (CI) around the accuracies and the differences, the sequence of analysis was repeated 1000 times. These repetitions were performed for each available combination in the sample assignment step.

## Ethical considerations
### Principle of autonomy
The dataset that was used in this study has been made freely reusable in Dryad Digital Repository [@dryadrepos]. Therefore, the principle of autonomy is upheld due to there not being any requirement for informed consent.

### Principle of beneficence
This study attempted to act in the best interest of future analytical research and patients, by developing and testing a new method for predicting prediction model performance after transfer utilizing unlabeled data. Such a method could in theory simplify the process of implementing prediction models in clinical practice and therefore, indirectly improve decision making changes in health care professionals, patient health outcomes and/or cost-effectiveness of care.

### Principle of nonmaleficence
The method developed in this study will be made without the intention of harm, intentionally or unintentionally. To nullify the risk of patient identification leakage, we used a dataset that has already been depersonalized and made freely reusable. By taking these actions we determined that the risk to the population is minimal.

### Principle of justice
Due to this study being analytical, the principle of justice does not prevail. However, the data in the study was treated equally.

### Ethical permit
No ethical permit was required because this study used a public database.

# Results
## Original Sample description
All 1303 participants (`r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,1][[1]]` `r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,2][[1]]`, `r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,1][[2]]` `r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,2][[2]]`, `r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,1][[3]]` `r data.frame(sort(table(pctabledf$Country), decreasing = TRUE))[,2][[3]]`) in the original dataset were used to simulated new participants. Out of these participants, `r raw.data$ICU[[1]]` were not admitted to the ICU and their mean age was `r round(mean(raw.data$age),2)` years with `r round(table(raw.data$gender)[[2]] / length(raw.data$gender) * 100, 2)` % being males. Baseline characteristics of the original dataset stratified by country are shown in table 1.

```{r, echo = FALSE}
kableone(Pctable, caption = 'Baseline characteristics of the original dataset stratified by country.')
```

## Simulation sample description
With the 1303 participants in the original dataset, `r length(list.simulated.dfs$Country) / 3` new participants were simulated for each country. The mean age of the participants in the simulated sample was `r round(mean(list.simulated.dfs$Age),2)` years. Baseline characteristics of the simulated samples stratified by country are shown in table 2.

```{r, echo = FALSE}
kableone(Pctablesim, caption = 'Baseline characteristics of the simulated samples stratified by country.')
```

## Development sample accuracies
We found the mean development sample accuracy to be `r dftable(5)[[2]][1]` % (95% CI `r round(cis[1+24],2)` to `r round(cis[1+36+24],2)`) within `r strata.combinations[[5]][1]`, `r dftable(2)[[2]][1]` % (95% CI `r round(cis[1+6],2)` to `r round(cis[1+36+6],2)`) within `r strata.combinations[[2]][1]` and `r dftable(1)[[2]][1]` % (95% CI `r round(cis[1],2)` to `r round(cis[1+36],2)`) within `r strata.combinations[[1]][1]`. All development sample accuracies with mean accuracies and CIs are shown in table 1 stratified by country.

```{r, echo = FALSE, fig.cap = 'Development sample accuracies in each country. Each black dot represents the development sample accuracy in one simulated sample with the red dot representing the mean accuracy. The bars indicate the 95% CI around the accuracies.'}
jitterplot(1)
```

## Validation sample accuracies
We found the mean validation sample accuracy to be `r dftable(5)[[2]][2]` % (95% CI `r round(cis[2+24],2)` to `r round(cis[2+36+24],2)`) within the `r strata.combinations[[5]][1]` to `r strata.combinations[[5]][2]` transfer, `r dftable(3)[[2]][2]` % (95% CI `r round(cis[2+12],2)` to `r round(cis[2+36+12],2)`) within the `r strata.combinations[[3]][1]` to `r strata.combinations[[3]][2]` transfer, `r dftable(2)[[2]][2]` % (95% CI `r round(cis[2+6],2)` to `r round(cis[2+36+6],2)`) within the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer, `r dftable(4)[[2]][2]` % (95% CI `r round(cis[2+18],2)` to `r round(cis[2+36+18],2)`) within the `r strata.combinations[[4]][1]` to `r strata.combinations[[4]][2]` transfer, `r dftable(1)[[2]][2]` % (95% CI `r round(cis[2],2)` to `r round(cis[2+36],2)`) within the `r strata.combinations[[1]][1]` to `r strata.combinations[[1]][2]` transfer and `r dftable(6)[[2]][2]` % (95% CI `r round(cis[2+30],2)` to `r round(cis[2+36+30],2)`) within the `r strata.combinations[[6]][1]` to `r strata.combinations[[6]][2]` transfer. All validation sample accuracies with mean accuracies and CIs are shown in table 1 stratified by transfer combination.

```{r, echo = FALSE, fig.cap = 'Validation sample accuracies stratified by transfer combination. Each black dot represents the validation sample accuracy in one simulated sample with the red dot representing the mean accuracy across all simulations. The bars indicate the 95% CI around the accuracies.'}
jitterplot(2)
```

## Predicted validation sample accuracies
We found that our method's predicted validation sample accuracy mean to be `r dftable(5)[[2]][3]` % (95% CI `r round(cis[3+24],2)` to `r round(cis[3+36+24],2)`) within the `r strata.combinations[[5]][1]` to `r strata.combinations[[5]][2]` transfer, `r dftable(3)[[2]][3]` % (95% CI `r round(cis[3+12],2)` to `r round(cis[3+36+12],2)`) within the `r strata.combinations[[3]][1]` to `r strata.combinations[[3]][2]` transfer, `r dftable(2)[[2]][3]` % (95% CI `r round(cis[3+6],2)` to `r round(cis[3+36+6],2)`) within the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer, `r dftable(4)[[2]][3]` % (95% CI `r round(cis[3+18],2)` to `r round(cis[3+36+18],2)`) within the `r strata.combinations[[4]][1]` to `r strata.combinations[[4]][2]` transfer, `r dftable(1)[[2]][3]` % (95% CI `r round(cis[3],2)` to `r round(cis[3+36],2)`) within the `r strata.combinations[[1]][1]` to `r strata.combinations[[1]][2]` transfer and `r dftable(6)[[2]][3]` % (95% CI `r round(cis[3+30],2)` to `r round(cis[3+36+30],2)`) within the `r strata.combinations[[6]][1]` to `r strata.combinations[[6]][2]` transfer. All predicted validation sample accuracies with mean accuracies and CIs are shown in table 3 stratified by transfer combination.

```{r, echo = FALSE, fig.cap = 'Predicted validation sample accuracies stratified by transfer combination. Each black dot represents the predicted validation sample accuracy in one transfer with the red dot representing the mean accuracy. The bars indicate the 95% CI around the accuracies.'}
jitterplot(3)
```

## Error in naive approach
We found the mean error in the naive approach to be `r dftable(5)[[2]][4]` (95% CI `r round(cis[4+24],2)` to `r round(cis[4+36+24],2)`) within the `r strata.combinations[[5]][1]` to `r strata.combinations[[5]][2]` transfer, `r dftable(3)[[2]][4]` (95% CI `r round(cis[4+12],2)` to `r round(cis[4+36+12],2)`) within the `r strata.combinations[[3]][1]` to `r strata.combinations[[3]][2]` transfer, `r dftable(2)[[2]][4]` (95% CI `r round(cis[4+6],2)` to `r round(cis[4+36+6],2)`) within the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer, `r dftable(4)[[2]][4]` (95% CI `r round(cis[4+18],2)` to `r round(cis[4+36+18],2)`) within the `r strata.combinations[[4]][1]` to `r strata.combinations[[4]][2]` transfer, `r dftable(1)[[2]][4]` % (95% CI `r round(cis[4],2)` to `r round(cis[4+36],2)`) within the `r strata.combinations[[1]][1]` to `r strata.combinations[[1]][2]` transfer and `r dftable(6)[[2]][4]` (95% CI `r round(cis[4+30],2)` to `r round(cis[4+36+30],2)`) within the `r strata.combinations[[6]][1]` to `r strata.combinations[[6]][2]` transfer. All errors in the naive apporach with mean errors and CIs are shown in table 4 stratified by transfer combination.

```{r, echo = FALSE, fig.cap = 'Errors in the naive approach stratified by transfer combination. Each black dot represents the error in the naive apporach for one transfer with the red dot representing the mean error. The bars indicate the 95% CI around the errors.'}
jitterplot(4)
```

## Error in segmented approach
We found the mean error in the segmented approach to be `r dftable(5)[[2]][5]` (95% CI `r round(cis[5+24],2)` to `r round(cis[5+36+24],2)`) within the `r strata.combinations[[5]][1]` to `r strata.combinations[[5]][2]` transfer, `r dftable(3)[[2]][5]` (95% CI `r round(cis[5+12],2)` to `r round(cis[5+36+12],2)`) within the `r strata.combinations[[3]][1]` to `r strata.combinations[[3]][2]` transfer, `r dftable(2)[[2]][5]` (95% CI `r round(cis[5+6],2)` to `r round(cis[5+36+6],2)`) within the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer, `r dftable(4)[[2]][5]` (95% CI `r round(cis[5+18],2)` to `r round(cis[5+36+18],2)`) within the `r strata.combinations[[4]][1]` to `r strata.combinations[[4]][2]` transfer, `r dftable(1)[[2]][5]` (95% CI `r round(cis[5],2)` to `r round(cis[5+36],2)`) within the `r strata.combinations[[1]][1]` to `r strata.combinations[[1]][2]` transfer and `r dftable(6)[[2]][5]` (95% CI `r round(cis[5+30],2)` to `r round(cis[5+36+30],2)`) within the `r strata.combinations[[6]][1]` to `r strata.combinations[[6]][2]` transfer. All errors in the segmented approach with mean errors and CIs are shown in table 5 stratified by transfer combination.

```{r, echo = FALSE, fig.cap = 'Errors in the segmented apporach stratified by transfer combination. Each black dot represents the error in the segmented approach for one transfer with the red dot representing the mean error. The bars indicate the 95% CI around the errors.'}
jitterplot(5)
```

## Approach comparison
We found significantly better performance in the segmented approach than in the naive approach in two of our six transfers with a mean error difference of `r dftable(5)[[2]][6]` (95% CI `r round(cis[6+24],2)` to `r round(cis[6+36+24],2)`) in the `r strata.combinations[[5]][1]` to `r strata.combinations[[5]][2]` transfer and `r dftable(1)[[2]][6]` (95% CI `r round(cis[6],2)` to `r round(cis[6+36],2)`) in the `r strata.combinations[[1]][1]` to `r strata.combinations[[1]][2]` transfer.

We found significantly worse performance in the segmented approach than in the naive approach in four of our six transfers with a mean error difference of `r dftable(3)[[2]][6]` (95% CI `r round(cis[6+12],2)` to `r round(cis[6+36+12],2)`) in the `r strata.combinations[[3]][1]` to `r strata.combinations[[3]][2]` transfer, `r dftable(2)[[2]][6]` (95% CI `r round(cis[6+6],2)` to `r round(cis[6+36+6],2)`) in the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer, `r dftable(4)[[2]][6]` (95% CI `r round(cis[6+18],2)` to `r round(cis[6+36+18],2)`) in the `r strata.combinations[[4]][1]` to `r strata.combinations[[4]][2]` transfer and `r dftable(6)[[2]][6]` (95% CI `r round(cis[6+30],2)` to `r round(cis[6+36+30],2)`) in the `r strata.combinations[[2]][1]` to `r strata.combinations[[2]][2]` transfer. All error differences with mean error differences and CIs are shown in figure 6 stratified by transfer combination.

```{r, echo = FALSE, fig.cap = 'Error differences between the naive approach and the segmented approach stratified by transfer combination. Each black dot represents the error difference between the naive and the segmented approach for one transfer with the red dot representing the mean error difference. The bars indicat the 95% CI around the error differences. Asterisk (*) above the mean error difference indicate statistical significance.'}
jitterplot(6)
```

# Discussion
## Key findings
At present, no methods exist that can predict the performance of a prediction model after transfer using unlabeled data. In this study, we have developed and tested such a method. We found that our method's predicted accuracy was significantly worse at predicting the accuracy of the prediction models after transfer than the development sample accuracy in four out of six transfer combinations, which suggest against our hypothesis that our method's predicted accuracy would be as good or better at predicting the accuracy after transfer as the development sample accuracy.

## Naive and segmented approach
We found that the accuracy of the prediction models were significantly worse after transfer when compared with the development sample accuracy. This was the case in five out of six transfer combinations, which suggest that our initial predictions were optimistic. These findings are consistent with the concluding remarks of Moons et al [@moons2012risk], that most prediction models perform worse when applied to individuals that differ from those used to train the prediction model. As an example, Ohnuma et al [@ohnuma2017prediction] showed similar losses in performance in the prediction models that they reviewed.

We also found that the accuracy of the prediction models were significantly worse after transfer when compared with our method's predicted accuracy. This was the case in four of our six transfer combinations, which suggest that our method's predicted accuracy were still optimistic. One reason why our method's predictions were still optimistic, could be that participants from the segmented samples were included in the training of the prediction models.

(detta kanske egentligen inte spelar någon roll på grund av hur logistisk regression fungerar, och att även om de vore overfittad till dessa samples bör ju accuracyn kanske fortfarande vara lika bra i de andra då de har liknande model prediktorer, vilket därav borde ha även mer liknande model utfall? gärna en kommnetar på detta. Vet att vi diskuterat detta tidigare, men behöver får det klar för mig)

Another reason why our method's predictions were still optimistic, could be that the model outcomes varied despite having more similar model predictors<!-- When you write predictors here it sounds like you mean the variables, not how they are distributed. I suggest that you revise to clarify that you're talking about how the predictor data was distribute. -->. Which indicates that even if the model predictors are more similar, it does not necessarily mean that the participants received similar ICU admission<!-- 1. Do not start a sentence with which. 2. This means that the predictor effects (associations) varied between settings. Try to make that more clear -->.

## Transfers where our method was better
Because most prediction models perform worse when they are introduced to new individuals, it would be beneficial for our method to predict an accuracy that is lower in most transfers when compared with the development sample accuracy. In the two transfer combinations where our method's predicted accuracy were significantly better than the development sample accuracy, this was the case. Although no significant difference was present between our method's predicted accuracy and the development sample accuracy in these two transfer combinations, the average of our method's predicted accuracy was lower.

One reason to why our method's predicted accuracy was better in these two transfer combinations could be due to a smaller proportion of the participants from the segmented samples being included in the training of the prediction models compared to the other transfer combinations. Another reason could be that the model outcomes in the segmented samples and the validation samples were more similar in these transfer combinations. Another reason could be that the number of missclassified validation samples was smaller in these transfer combinations, which would make the segmented samples a better match to the validation samples. We did not however measure how large the missclassified validation samples were in this study.
**Finns det något mer här? kommer verkligen inte på något annat..**

## Other methods
Although this is the first method that predicts the performance of a prediction model after it has been transferred using unlabeled data, methods exist that predict the performance of a prediction model in populations that are similar to the development sample. These methods do not utilize unlabeled data and some of these methods include different types of bootstrapping, cross-validation and split-sampling [@efron1983estimating ; @efron1994introduction ; @efron1997improvements ; @picard1990data]. Steyerberg et al [@steyerberg2001internal] showed that regular bootstrapping is to be preferred when predicting the performance of a prediction model within similar populations. In this study we did not compare our method with these methods, but we did compare it with the development sample accuracy.
**Borde jag lägga till något mer här?**<!-- If you want to make more references to other research you may write something about optimism and how that can be estimated and adjusted for using for example bootstrapping -->

## Strengths and limitations
The strength of our study is that to the best of our knowledge, this is the first study to develop and test a method that predicts the performance of a prediction model after transfer using unlabeled data. Such a method could provide information about how a prediction model would perform after it has been transferred. This information could be used to simplify the implementation of the prediction model and therefore indirectly improve decision making of health care professionals, patient health outcome and/or cost effectiveness of care.

Our study also has limitations. First, due to having small sample sizes in some of our countries, we decided to simulate new samples. These new samples were likely poor representatives of real samples. This is due to the model outcomes being simulated using logistic regression models that were trained with these small samples. This likely resulted in varying quality of the model outcomes in our different countries, which could have reflected poorly on our method. To avoid this limitation in future studies, larger sample should be used.

Second, due to being limited by time, we chose, based on simplicity, to only assess the performance of the prediction models in terms of the number of correct predictions compared to the total number of predictions. This was also done just for one specific decision threshold. In future studies, it would be beneficial to test our method with different measurements of performance. One example is the area under the receiver operating characteristic curve, which is independent of the decision threshold.

Third, due to not assessing the proportion of participants that were used to train the prediction models from the segmented samples, we do not know how this may have affected our method's predicted accuracy. It would therefore be interesting in future studies to test how our method's predicted accuracy is affected depending on this proportion.

Fourth, there is no consensus on which method is the best to use when choosing model predictors, but it has been recommended that all available model predictors should be included [@royston2009prognosis]. In our case, we included all available model predictors that were continuous. This may have created selection bias as we did not include categorical model predictors. 

Last, due to being limited by time, we decided to test our method with only one dataset with model predictors and outcomes from three countries. To get a better understanding of how our method performs more generally, it would be beneficial to test our method with multiple datasets and prediction models with data from multiple countries.

<!-- Move your suggestions for how to improve on these limitations to the section on future studies -->

## Significance
Because we have found results that contradict our hypothesis, we do not recommend that our method is used to predict the accuracy of a prediction model after transfer to another country. More research is needed to provide more firm recommendations. Because our method's predicted accuracy was better in two of our six transfers, we believe that there is scope to improve it. 

1) maybe retrain a prediction model while removing the identified segmented sampels from the training of the model?
**jag kommer seriöst inte på fler**

## Future studies
A study that improves on the limitations in our study. By increasing the sample sizes, testing our method with different measurements of performance and assessing the proportion of participants in the segmented samples that are used to train the prediction models. Doing so with several datasets with different prediction models would give a better understanding of how and if our method is better at predicting the accuracy after transfer.

A study that compares our method with other methods that exist for predicting the performance of a prediction model in similar populations. Although these other methods are not developed for predicting the performance after transfer, it would be interesting to assess whether our method, that requires unlabeled data to be collected, is better or not.

# Conclusions
Although our method used unlabeled data to predict the accuracy of a prediction model after transfer, our results do not support the use of our method when compared with the accuracy of the prediction model within the country in which it was developed. However, our study has several limitations that may have reflected poorly on our method. Therefore, further studies on this method are needed to get a better idea of how it works.

# References
